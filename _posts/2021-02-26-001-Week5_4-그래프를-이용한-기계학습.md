---
title: Week5_4 그래프를 이용한 기계학습
categories: [boostcamp]
tags: [부스트캠프, Graph]
math: true
comments: true
---

\- 이 강의정리본은 KAIST 신기정 교수님의 강의를 정리한 것임을 밝힙니다.\- 

### 정점 표현 학습

 #### 정점 표현 학습이란?

- 그래프의 정점들을 벡터의 형태로 표현하는 것.
- 간단히 `정점 임베딩(Node Embedding)`이라고도 부름.
- 정점이 표현되는 벡터공간을 `임베딩 공간`이라고 부른다.
- ![](/assets/img/post_images/image-20210226160655361.png)
- `입력` = 그래프
- `출력` = 각 정점 `u`에 대한 임베딩 $$z_u$$
- ![](/assets/img/post_images/image-20210226160807473.png)
- $$f$$ : 함수. 입력으로 그래프의 정점을 하나 받아서, 벡터를 반환하는 함수!
  - 출력 차원을 `d`로 표현한다.

#### 정점 표현 학습의 이유

- 정점 임베딩 덕분에, 벡터 형태의 데이터를 위한 `도구들`을 그래프에도 적용할 수 있다.
  - 예를 들어, 기계학습 도구들이 있다.
  - 대부분의 분류기(Logistic Regression, 퍼셉트론 등)
  - 군집 분석 알고리즘(K-Means, DBSCAN)
  - 벡터 형태로 입력을 받는다.
  - 따라서, 이러한 도구들을 정점 분류(Node Classification), 군집 분석(Community Detection) 등에 활용할 수 있다는 것!!
  - 지금까지의 수업은, 그래프 형태의 데이터를 처리하기 위해, 특화된 별도의 알고리즘을 만들었지만, 
  - 그래프의 정점들을 벡터들로 잘 표현할 수 있다면 이러한 도구들을 활용할 수 있다.

#### 정점 표현 학습의 목표

- 어떤 `기준`으로 정점을 벡터로 변환해야할까? 

- 목표 : 그래프에서의 정점간 유사도를 임베딩 공간에서도 '보존'하는 것!

- ![](/assets/img/post_images/image-20210226161215097.png)

- 위의 그래프에서 보라색 정점들은 유사함. 오른쪽 벡터공간에서도 유사한 벡터로 표현된것을 확인할 수 있다!

- 임베딩 공간에서의 유사도 : `내적(inner product)`을 사용함.

  - 내적은 ①두 벡터가 클수록, 그리고 ②같은 방향을 향할수록 큰 값을 갖는다.

  - `u`와 `v`의 내적은

  - $$
    z_v^Tz_u = ||z_u||*||z_v||*cos(\theta)
    $$

- 정리하면, 정점 임베딩은 두 단계로 이루어진다.

  - ① 그래프에서의 정점 유사도를 정의하는 단계
  - ② 정의한 유사도를 보존하도록 정점 임베딩을 학습하는 단계

### 첫 번째 접근법: 인접성 기반 접근법

#### 인접성 기반 접근법

- 두 정점이 `인접`할 때 `유사`하다고 간주한다!
- '두 정점 `u`와 `v`가 인접하다' → 둘을 직접 연결하는 간선`(u,v)`가 있음을 의미한다.
- 인접행렬 A의 `u`행 `v`열 원소 $$A_{u, v}$$는 `u`와 `v`가 인접한 경우 `1`, 아닌 경우 `0`
  - $$A_{u, v}$$를 `u`와 `v`의 유사도로 가정한다.
- ![](/assets/img/post_images/image-20210226163535800.png)
- 위 손실함수가 최소가 되는 정점 임베딩(`z`)을 찾는 것을 목표로 한다.
- 이를 최소화 하기 위해 경사하강법 등이 사용된다.

#### 인접성 기반 접근법의 한계

- 인접성만으로 유사도를 판단하는 것은 한계가 있다.
- ![](/assets/img/post_images/image-20210226163736898.png)
- 위의 그림에서, 빨간색 정점과 파란색 정점은 거리가 `3`인 반면, 초록색 정점과 파란색 정점은 거리가 `2`이다.
- 그러나, 인접성으로 접근하는 경우, 빨-파도 유사도가 0, 초-파도 유사도가 0이다.
- 군집 관점에서도, 가운데 회색 점을 중심으로 왼쪽 군집과 오른쪽 군집을 나누는 경우,
  - 빨간색 정점과 파란색 정점은 다른 군집에 속하지만,
  - 파란색 정점과 초록색 정점은 같은 군집에 속한다.

### 두 번째 접근법: 거리/경로/중첩 기반 접근법

- 인접성 기반 접근법의 한계를 극복하고자 다양한 접근법이 제시되었음.

#### 거리 기반 접근법

- 두 정점 사이의 거리가 충분히 가까운 경우 유사하다고 간주한다.
- ![](/assets/img/post_images/image-20210226164028471.png)
- "충분히"의 기준을 `2`로 정의하는 경우
  - 빨간색 정점은 초록색/파란색 정점들과 유사함. (유사도 `1`)
  - 빨간색 정점은 보라색 정점과는 유사하지 않음.(유사도 `0`)

#### 경로 기반 접근법

- 두 정점 사이의 `경로`가 많을 수록 유사하다고 간주한다.
- ![](/assets/img/post_images/image-20210226164324058.png)
- 두 정점 `u`와 `v` 사이의 경로 중 거리가 `k`인 것의 수는 $$A_{u,v}^k$$와 같다.
  - 즉, 인접행렬 A의 `k`제곱의 `u`행 `v`열 원소와 같다.
  - 경로기반접근법의 손실함수는 위의 식과 같다.

#### 중첩 기반 접근법

- 두 정점이 많은 이웃을 공유할수록 유사하다고 간주함.

- ![](/assets/img/post_images/image-20210226164619442.png)

- 위에서 빨간색 정점과 파란색 정점은 이웃을 `2`명 공유하고 있으므로, 유사도도 `2`이다. 

- 마찬가지로 왼쪽 위 회색 정점과 파란색 정점은 이웃을 `1`명 공유하므로 유사도도 `1`이다.

- 왼쪽 위 회색 정점과 맨 오른쪽 회색 정점은 공유하는 이웃이 없으므로 유사도는 `0`이다.

- 정점 `u`의 이웃 집합을 $$N(u)$$, 그리고 정점 `v`의 이웃 집합을 $$N(v)$$라 하면

  - 두 정점의 공통 이웃 수 $$S_{u, v}$$는 다음과 같이 정의된다.

  - $$
    S_{u,v} = |N(u)\cap N(v)| = \sum_{w\in N(u) \cap N(v)}1
    $$

- 중첩 기반 접근법의 손실함수는 다음과 같다.

- ![](/assets/img/post_images/image-20210226165016122.png)

- 즉, 손실함수에 `u`와 `v`의 공통 이웃의 수가 들어가게 된다.

- 공통 이웃 수 대신에 ① 자카드 유사도 혹은 ② Adamic Adar 점수를 사용할 수도 있다.

  - 자카드 유사도는 공통 이웃의 수 대신 비율을 계산하는 방식이다.

  - $$
    \frac{N(u) \cap N(v)}{N(u) \cup N(v)} = \frac{u, v와\ 공통으로\ 인접한\ 정점의\ 수}{u \ 혹은\ v와\ 인접한\ 정점의\ 수}
    $$

    - 자카드 유사도가 `1`의 값을 갖는 경우는, $$N(u)=N(v)$$가 되는 경우이다.

  - Adamic Adar 점수는 공통 이웃 각각에게 가중치를 부여하여 가중합을 계산하는 방식이다.

  - $$
    \sum_{w\in N(u) \cap N(v)}\frac{1}{d_w}
    $$

    - `u`와 `v`의 공통 이웃 `w`에 대해서, `w`의 연결성이 크면 클수록 낮은 점수를 갖는다.
    - 왜일까? 
    - 트위터 계정 `u`와 `v`가 있다고 하자. 이 둘은 모두 트와이스 계정을 팔로우하고 있다.
    - 그러나 트와이스 계정은 엄청난 팔로워들을 가지고 있다. 
    - 따라서 `u`와 `v`가 트와이스를 팔로우한다는 사실이, 곧 `u`와 `v`가 가까운 것을 의미하지 않는다는 뜻!

### 세 번째 접근법: 임의보행 기반 접근법

#### 임의보행 기반 접근법

- 한 정점에서 시작하여 임의보행을 할 때 다른 정점에 도달할 확률을 유사도로 간주한다.

- `임의보행` : 현재 정점의 이웃 중 하나를 균일한 확률로 선택하여 이동하는 과정을 반복하는 것.

- 임의 보행을 사용할 경우, 시작 정점 주변의 `지역적 정보`와 `그래프 전역 정보`를 모두 고려한다는 장점이 있다.

- ![](/assets/img/post_images/image-20210226170255949.png)

- 그래프 전역 정보를 사용한다는 것은 어떤 의미일까?

  - 거리/경로기반 접근법에서는 거리를 `k`로 제한했었으나, 임의보행 접근법에서는 거리를 제한하지 않는다. 그런 점에서 전역 정보를 고려한다고 할 수 있다.

- 임의보행 접근법은 세 단계를 거친다.

  - ① 각 정점에서 시작하여 임의보행을 반복 수행한다.

  - ② 각 정점에서 시작한 임의보행 중 도달한 정점들의 리스트를 구성함.

    - 이 때, 정점 `u`에서 시작한 임의보행 중 도달한 정점들의 리스트를 $$N_R(u)$$라고 하자.
    - 한 정점을 여러번 도달한 경우, 해당 정점은 $$N_R(u)$$에 여러 번 포함될 수 있다.
    - 다음 손실함수를 최소화하는 임베딩을 학습한다.
    - ![](/assets/img/post_images/image-20210226171345934.png)
    - 확률값이 크면 클수록 좋다.

  - 어떻게 임베딩으로부터 도달 확률을 추정할 수 있을까?

  - $$
    p(v|z_u) = \frac{exp(z_u^Tz_v)}{\sum_{n\in V}exp(z_u^Tz_n)}
    $$

  - 즉, 유사도 $$z_u^Tz_n$$이 높을수록 도달확률이 높다. 

  - 이를 대입해서 다시 손실함수식을 보면,

  - ![](/assets/img/post_images/image-20210226171636668.png)

#### DeepWalk와 Node2Vec

- 임의보행의 방법에 따라 `DeepWalk`와 `Node2Vec`이 구분된다.

  - `DeepWalk`는 앞서 설명한 기본적인 임의보행을 사용한다. 즉, 현재 정점의 이웃 중 하나를 균일한 확률로 선택하여 이동하는 과정을 거친다.

  - `Node2Vec`은 `2차 치우친 임의보행`(2nd-order Biased Random Walk)을 사용한다. 즉, 현재 정점과, 직전에 머물렀던 정점을 모두 고려하여 다음 정점을 선택한다.

    - ![](/assets/img/post_images/image-20210226171930702.png)

    - 직전 정점의 거리를 기준으로 경우를 구분하여 차등적인 확률을 부여한다.
    - `u`에서 출발한 정점이 `v`에 도착했다고 하자. 
      - 이 때 , 다시 `u`로 가는 것은 이전 정점으로 가는 것이므로 가까워지는 방향이다.
      - `x`로 가는 것은 거리가 유지되는 방향이다. 왜냐하면, 직전 정점인 `u`를 기준으로 `v`와 ,`x` 모두 거리가 `1`이기 때문이다.
      - 한편 ,`y`로 간다는 것은 이전 정점 `u`보다 더욱 멀어지는 것을 의미한다. 
    - `Node2Vec`에서는 부여하는 확률에 따라 다른 종류의 임베딩을 얻는다. 
    - ![](/assets/img/post_images/image-20210226172329105.png)
    - 위의 그림에서
      - 멀어지는 방향에 높은 확률을 부여한 경우, 파란색 정점들은 서로 다른 군집들을 연결해주는 다리 역할을 하는 정점들임을 알 수 있다. 또한, 노란색 정점들은 그래프의 주변부에 위치하는 정점들임을 알 수있다. 빨간색 정점들은 같은 군집에 속하는 정점들이다.
      - 즉, 정점들이 `역할`을 기준으로 임베딩된 것을 확인할 수 있다.
      - 가까워지는 방향에 높은 확률을 부여한 경우, 같은색깔의 정점들은 같은 `군집`에 속해있는 것을 볼 수 있다. 

#### 손실함수근사

- 임의보행 기법의 손실함수는 `정점 수의 제곱`에 비례하는 시간이 소요된다. 이는 중첩 합($$\sum \sum$$) 때문이다.

- 따라서, 많은 경우 근사식을 사용한다.

  - 모든 정점에 대해 정규화하는 대신, 몇 개의 정점을 뽑아서 비교하는 형태.
  - 이 때, 뽑힌 정점들을 네거티브 샘플이라고 부른다. 
  - 연결성에 비례하는 확률로 네거티브 샘플을 뽑으며, 네거티브 샘플이 많을수록 학습이 더욱 안정적이다.

- 근사식

  - $$
    log(\frac{exp(z_u^Tz_v)}{\sum_{n\in V}exp(z_u^Tz_n)})
    $$

  - $$
    \approx log(\sigma(z_u^Tz_v) - \sum_{i=1}^klog(\sigma(z_u^Tz_{n_i})), n_i\sim P_V
    $$

### 변환식 정점 표현 학습의 한계

#### 변환식 정점 표현 학습과 귀납식 정점 표현 학습

#### 변환식 정점 표현 학습의 한계

### 실습: Node2vec을 사용한 군집 분석과 정점 분류

