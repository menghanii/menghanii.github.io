---
title: 추천시스템 1
tag: [추천시스템]
categories: [추천시스템]
math: true
comments: true
---

## 좋은 Recommendation System이란?

---

좋은 추천이란 무엇인가? 혹은 좋은 추천 전략은 무엇인가? 혹은 내 비즈니스를 위해 좋은 추천 전략은 무엇인가? 일반적으로 실무에서 좋은 추천전략임을 파악하는데 사용되는 지표는

① 클릭율(Click-through-rate: CTR)
② 구매율(전환율, Conversion rate: CR)
③ 매출 증가(sales increase)
④ 체류기간(Interactivity on Platform)
⑤ 재구매율(Customer return rates)
⑥ 만족도와 충성도(Customer satisfaction and loyalty)

가 있다.

추천에 사용하는 데이터의 종류는 크게 **명시적 데이터** 와 **묵시적 데이터**로 나눠지는데, 명시적 데이터란 별점, 점수, 좋아요, 구독 등 사용자들이 명확하게 자신의 취향이나 선호도를 표시한 데이터를 의미한다. 반면 묵시적 데이터는 '추론할 수 있는' 데이터로, 사용자들의 행동 패턴 등을 통해 이들의 선호도를 간접적으로 유추할 수 있는 데이터이다. 예를 들어, 사용자가 특정 제품 페이지에서 머무르는 시간이 길었다든지, 해당 제품 페이지를 자주 방문했다든지 하는 지표는 간접적으로 해당 사용자가 특정 제품에 관심이 있다는 것을 의미한다.

## 추천시스템 평가

---

추천 시스템을 평가하는 방법도 두 가지로 나눠볼 수 있다.

첫 번째는, 실험(Lab experiment)이다. 이는 평가의 목적으로 만들어진 통제된 환경 안에서 추천시스템을 평가하는 방식이다. 변수가 통제될 수 있다는 점에서는 보고자 하는 변수의 명확한 양상을 알 수 있어서 좋지만, 실제로 피실험자는 물건을 구매하지도 않을 뿐더러, 실험적인 환경 속에서 얼마나 현실적으로 행동할지에 대해 확신하기 어렵다. 

두 번째는, 실제 현장을 평가하는 것이다.(Field Study) 가장 대표적인 방법으로는 A/B Test 가 있다. 본질적으로 대조 실험과 같다. 가설 입증을 위해 대조군과 실험군을 설정하고 결과를 검증하는 것이다. A/B Testing은 이 방법론을 인터넷 환경에 맞게 실행하는 것으로 볼 수 있다.

예를 들어, "구매 버튼 옆에 '당신이 관심있어할 만한 제품' 배너를 놓는 경우 매출이 20% 증가한다." 하는 것이 A/B Test의 결과로 볼 수 있다.  

## 추천시스템의 정확도 평가

---

일반적인 머신 러닝 평가 방법과 동일한 방법을 사용한다.

즉, Train / Test Set 을 나누고, Train을 통해 나온 모델의 성능을 Test Set의 정확도를 통해 검증해보는 것이다.

다만, 약간 다른 점은 추천시스템에서는 일반적으로 row에 User를, column에 Item을 놓는 행렬식이 적용되기 때문에, Train/Test Set의 구조가 다르게 생겼다. 

일반적인 Train / Test set의 구조는 train과 test set이 모두 같은 feature를 공유하면서 row만 분할되는 형태이다.

![train_test](https://user-images.githubusercontent.com/37925813/96999775-b5781b80-1570-11eb-9fb0-65ae204dbc33.png){:width="400px"}

그러나 추천 시스템에 있어서 column에는 아이템이 들어가기 때문에, train / test가 row와 column 모두에서 분할된다.

![RS_traintest](https://user-images.githubusercontent.com/37925813/96999769-b3ae5800-1570-11eb-85d7-f74512867094.png){:width="400px"}



### 정확도 측정지표

---

① 연속 값을 사용하는 경우에는 MAE, MSE, RMSE, NRMSE(Normalized RMSE : 실제치와 예측치를 정규화하여 RMSE를 구한다. Scale에 영향받지 않는다는 장점이 있다.)

② 한편, Binary 방식으로 classification 하는 경우에는, 흔히 많이들 쓰는 classification report 등을 통해 Precision / Recall score를 구하고, 이들을 통해 AUC Score 혹은 F1-Score를 구한다. 일반적으로 Precision과 Recall은 상충관게에 있어서, 하나가 증가하면 다른 하나는 감소하는 경향이 있다.

F1-Score는 Precision과 Recall을 통해 하나의 값으로 나타내기 위한 스코어인데,



$$
F_1 = 2\frac{precision*recall}{precision + recall}
$$



로 나타내어진다.

F1-Score는 precision과 recall에 동일한 weight값을 부여하는 것을 알 수 있다.

③ 다음으로는 **Ranking** 으로 추천해주는 경우도 있다. 이 경우에도 적합하게 rank했는지를 평가하는 metrics가 있는데, precision과 recall의 확장 개념이라고 생각하면 된다. 먼저 **Average Precision **. 예를 들어, 특정 아이템과 상관성이 가장 높은 아이템 5위까지를 뽑는 과정에서, 3개를 맞췄다고 하자. 맞춘 놈의 순위는 [1등, 4등, 5등]이 될 수도 있고, [2등, 3등, 4등]이 될 수도 있고, 여러 경우의 수가 있을 것이다. 

AP는 첫 번째 예측에 대해서



$$
AP = \frac{1}{3}(\frac{1}{1} + \frac{2}{4} + \frac{3}{5}) = 0.7
$$



의 Score를 줄 것이고, 두 번째의 예측에 대해서는



$$
AP = \frac{1}{3}(\frac{1}{2} + \frac{2}{3} + \frac{3}{4}) = 0.639
$$



를 줄 것이다. 일종의 weight가 들어간 평균 점수라고 이해가 된다. 두 점수의 차이는, 분자는 1, 2, 3 으로 동일하지만, 분모가 다른 데에 있다. 전자는 1등을 잘맞춰서 후자보다 점수가 높다. 즉 AP는 **상관성이 가장 높은 아이템**을 맞추는 것이 우선 중요하다고 생각한다. 

![image-20201023210106815](https://user-images.githubusercontent.com/37925813/97001167-e3f6f600-1572-11eb-8f74-354313819a0e.png){:width="400px"}



④ 그 외의 정확도 측정 지표로는 Coverage(user-item pair 중 예측할 수 있는 pair의 비율), Novelty(추천된 아이템의 신선함), CTR, CR 등이 있다.